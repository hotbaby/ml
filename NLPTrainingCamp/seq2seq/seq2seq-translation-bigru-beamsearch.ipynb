{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq模型——机器翻译\n",
    "\n",
    "高级改进：\n",
    "\n",
    "beamsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import jieba\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE_CUDA: True\n"
     ]
    }
   ],
   "source": [
    "print('USE_CUDA: %s' % USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "print('device: %s' % device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENTATION = False    # 是否分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本预处理\n",
    "\n",
    "丢弃除了中文、字母和常用标点之外的符号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z\\u4e00-\\u9fa5.!?，。？]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建词表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入三个特殊的Token:\n",
    "\n",
    "1. `SOS`, \"Start of sentence”，标识句子开始\n",
    "2. `EOS`, “End of sentence”，表示句子结束\n",
    "3. `UNK`, \"Unknown Token\"，标识未登录词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "UNK_token = 2\n",
    "\n",
    "class Lang(object):\n",
    "    \"\"\"\n",
    "    词表Vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", '2': 'UNK'}\n",
    "        self.n_words = 3 # Count SOS and EOS\n",
    "      \n",
    "    def index_words(self, sentence):\n",
    "        if self.name == 'cn':\n",
    "            words = list(jieba.cut(sentence)) if SEGMENTATION else sentence    \n",
    "            for word in words:\n",
    "                self.index_word(word)\n",
    "        else:\n",
    "            words = sentence.split(' ')\n",
    "            for word in words:\n",
    "                self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取平行语料，并进行清理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('%s-%s.txt' % (lang1, lang2)).read().strip().split('\\n')\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据集\n",
    "\n",
    "样例为了加快训练，只保留了不长于10个单词的句对，真正实验中将更多数据考虑进来可能获得更好的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "def filter_pair(p):\n",
    "    return len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if filter_pair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理数据的全过程：\n",
    "\n",
    "- 读取数据，每一行分别处理，将其转换成句对\n",
    "- 对于文本进行处理，过滤无用符号\n",
    "- 根据已有文本对于单词进行编号，构建符号到编号的映射\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 90000 sentence pairs\n",
      "Trimmed to 68898 sentence pairs\n",
      "Indexing words...\n",
      "['汤姆不尊重玛丽的观点。', 'tom doesn t respect mary s opinion .']\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data('cn', 'eng', False)\n",
    "\n",
    "# Print an example pair\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从数据集中sample出200条数据作为验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_test_dataset(size=100):\n",
    "\n",
    "    with open('cn-eng-test.txt', 'w+') as f:\n",
    "        f.write('\\n'.join(['\\t'.join(pair) for pair in random.sample(pairs, k=size)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_test_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将文本数据转换为张量\n",
    "\n",
    "为了训练，我们需要将句子变成神经网络可以理解的东西（数字）。每个句子将被分解成单词，然后变成张量，其中每个单词都被索引替换（来自之前的Lang索引）。在创建这些张量时，我们还将附加EOS令牌以表示该句子已结束。\n",
    "\n",
    "![](https://i.imgur.com/LzocpGH.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    \"\"\"\n",
    "    根据词表，将句子转化成索引列表。\n",
    "\n",
    "    :reutrn list，e.g. [1, 2, 3, 4]\n",
    "    \"\"\"\n",
    "    if lang.name == 'cn':\n",
    "        words = list(jieba.cut(sentence)) if SEGMENTATION else sentence\n",
    "        return [lang.word2index[word] if word in lang.word2index else UNK_token for word in words ]\n",
    "    else:\n",
    "        words = sentence.split(' ')\n",
    "        return [lang.word2index[word] if word in lang.word2index else UNK_token for word in words]\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    \"\"\"\n",
    "    将句子转换成Tensor.\n",
    "    \n",
    "    :return Tensor, shape(n, 1)\n",
    "    \"\"\"\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    var = torch.LongTensor(indexes).view(-1, 1)\n",
    "    if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "def variables_from_pair(pair):\n",
    "    \"\"\"\n",
    "    将平行语料对转化成Tensors.\n",
    "    \n",
    "    :return (input_tensor, output_tensor)\n",
    "    \"\"\"\n",
    "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
    "    target_variable = variable_from_sentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair: ['我在这里闻见了什么。', 'i smell something here .']\n",
      "input_tensor shape: torch.Size([11, 1]), output_tensor shap: torch.Size([6, 1])\n",
      "input_tensor: tensor([[   3],\n",
      "        [  15],\n",
      "        [ 138],\n",
      "        [  70],\n",
      "        [1854],\n",
      "        [ 742],\n",
      "        [  12],\n",
      "        [  17],\n",
      "        [ 255],\n",
      "        [  13],\n",
      "        [   1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "pair = random.choice(pairs)\n",
    "print('pair: %s' % pair)\n",
    "\n",
    "input_tensor, target_tensor = variables_from_pair(pair)\n",
    "print('input_tensor shape: %s, output_tensor shap: %s' % (input_tensor.shape, target_tensor.shape))\n",
    "print('input_tensor: %s' % input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class EncoderGRU(nn.Module):\n",
    "    \"\"\"GRU 编码器\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, bidirectional=False):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param input_size, 输入词表大\n",
    "        :param hidden_size, Embedding维度大小，RNN hidden大小\n",
    "        :param n_layers, RNN层数\n",
    "        \"\"\"\n",
    "        super(EncoderGRU, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        # 用GRU替换RNN\n",
    "        # self.rnn = nn.RNN(hidden_size, hidden_size, n_layers)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, n_layers, bidirectional=bidirectional)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        :param word_inputs, 输入序列 shape(n, 1)\n",
    "        :param hidden, 隐层 shape(seq_len*n_layers, batch_size, hidden_size)\n",
    "        :return output(seq_len, batch, num_directions*hidden_size),\n",
    "                hidden(num_layers*num_directions, hidden_size)\n",
    "        \"\"\"\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        hidden = torch.zeros(self.n_layers*num_directions, 1, self.hidden_size)\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class DecoderGRU(nn.Module):\n",
    "    \"\"\"GRU 解码器\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(DecoderGRU, self).__init__()\n",
    "        \n",
    "        # Keep parameters for reference\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "\n",
    "        # 使用GRU替换RNN\n",
    "        # self.rnn = nn.RNN(hidden_size, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, word_input, last_hidden):\n",
    "        # Note: we run this one step at a time        \n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        rnn_output, hidden = self.rnn(word_embedded, last_hidden)\n",
    "\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        output = F.log_softmax(self.out(rnn_output))\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.load('model_storage/nmt-seq2seq-bigru-encoder.pkl')\n",
    "decoder = torch.load('model_storage/nmt-seq2seq-bigru-decoder.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence, max_length=MAX_LENGTH):\n",
    "    input_variable = variable_from_sentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = torch.LongTensor([[SOS_token]]) # SOS\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    decoder_hidden = encoder_hidden.view(encoder.n_layers, encoder_hidden.shape[1], -1)\n",
    "    \n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('EOS')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni.item()])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = torch.LongTensor([[ni]])\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "\n",
    "class BeamSearchNode(object):\n",
    "    \"\"\"Beamsearch Node\"\"\"\n",
    "    \n",
    "    def __init__(self, decoder_input, decoder_hidden, score, decode_words=None):\n",
    "        self.decoder_input = decoder_input\n",
    "        self.decoder_hidden = decoder_hidden\n",
    "        self.decode_words = decode_words or []\n",
    "        self.score = score\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'BeamNode score: %s, words: %s' % (self.score, ' '.join(self.decode_words))\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_bs(sentence, max_length=MAX_LENGTH,  bs_size=2, debug=False):\n",
    "    \"\"\"使用beamsearch提高预测能力.\"\"\"\n",
    "    \n",
    "    # 队列缓存BeamSeach Node\n",
    "    q = queue.Queue()\n",
    "\n",
    "    input_variable = variable_from_sentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = torch.LongTensor([[SOS_token]]) # SOS\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    decoder_hidden = encoder_hidden.view(encoder.n_layers, encoder_hidden.shape[1], -1)\n",
    "    \n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    # First Node\n",
    "    bs_node = BeamSearchNode(decoder_input, decoder_hidden, 1.0)\n",
    "    q.put(bs_node)\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        if q.empty():\n",
    "            break\n",
    "\n",
    "        # 候选集\n",
    "        candidates = []\n",
    "        while not q.empty():\n",
    "            bs_node = q.get()\n",
    "        \n",
    "            if bs_node.decode_words and bs_node.decode_words[-1] == 'EOS':\n",
    "                candidates.append(bs_node)\n",
    "                continue\n",
    "\n",
    "            decoder_output, decoder_hidden = decoder(bs_node.decoder_input, bs_node.decoder_hidden)\n",
    "\n",
    "            # Choose top word from output\n",
    "            # topv: top values\n",
    "            # topi: top value indices\n",
    "            topv, topi = decoder_output.data.topk(bs_size)\n",
    "            for i in range(bs_size):\n",
    "                ni = topi[0][i]\n",
    "                score = abs(topv[0][i].item())   # Note: 解码器输出的是log_softmax\n",
    "                decode_word = output_lang.index2word[ni.item()]\n",
    "                decoder_input = torch.LongTensor([[ni]])\n",
    "                if USE_CUDA:\n",
    "                    decoder_input = decoder_input.cuda()\n",
    "\n",
    "                if debug:\n",
    "                    print('score: %.6f, words: %s' % (\n",
    "                        bs_node.score * score, ' '.join(bs_node.decode_words + [decode_word])))\n",
    "\n",
    "                candidates.append(BeamSearchNode(decoder_input, \n",
    "                                                 decoder_hidden,\n",
    "                                                 bs_node.score*score,\n",
    "                                                 bs_node.decode_words[:] + [decode_word]))\n",
    "\n",
    "        if debug: print('\\n')\n",
    "        # 排序\n",
    "        candidates = sorted(candidates, key=lambda x: x.score)\n",
    "        for i in range(bs_size):\n",
    "            q.put(candidates[i])\n",
    "\n",
    "    return candidates[0].decode_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my father is only fifteen years old . EOS\n",
      "our father is only ten years old . EOS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotbaby/anaconda3/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(evaluate_bs('我的爸爸只有十五岁。')))\n",
    "print(' '.join(evaluate_bs('我们的爸爸只有十五岁。')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my father is only fifteen . EOS\n",
      "our only is is just . EOS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotbaby/anaconda3/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(evaluate('我的爸爸只有十五岁。')))\n",
    "print(' '.join(evaluate('我们的爸爸只有十五岁。')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.051014, words: our\n",
      "score: 3.799890, words: my\n",
      "\n",
      "\n",
      "score: 0.068516, words: our only\n",
      "score: 0.117402, words: our father\n",
      "score: 2.397962, words: my father\n",
      "score: 8.895734, words: my only\n",
      "\n",
      "\n",
      "score: 0.059149, words: our only is\n",
      "score: 0.216661, words: our only only\n",
      "score: 0.029143, words: our father is\n",
      "score: 0.374103, words: our father has\n",
      "\n",
      "\n",
      "score: 0.017970, words: our father is only\n",
      "score: 0.075150, words: our father is just\n",
      "score: 0.079870, words: our only is is\n",
      "score: 0.132378, words: our only is only\n",
      "\n",
      "\n",
      "score: 0.047692, words: our father is only ten\n",
      "score: 0.061728, words: our father is only one\n",
      "score: 0.206803, words: our father is just ten\n",
      "score: 0.209009, words: our father is just years\n",
      "\n",
      "\n",
      "score: 0.040267, words: our father is only ten years\n",
      "score: 0.121811, words: our father is only ten .\n",
      "score: 0.118071, words: our father is only one years\n",
      "score: 0.240102, words: our father is only one of\n",
      "\n",
      "\n",
      "score: 0.015129, words: our father is only ten years .\n",
      "score: 0.093163, words: our father is only ten years old\n",
      "score: 0.113608, words: our father is only one years .\n",
      "score: 0.154920, words: our father is only one years old\n",
      "\n",
      "\n",
      "score: 0.003569, words: our father is only ten years . EOS\n",
      "score: 0.024370, words: our father is only ten years . .\n",
      "score: 0.002413, words: our father is only ten years old .\n",
      "score: 0.351159, words: our father is only ten years old EOS\n",
      "\n",
      "\n",
      "score: 0.000030, words: our father is only ten years old . EOS\n",
      "score: 0.010712, words: our father is only ten years old . .\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotbaby/anaconda3/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'our father is only ten years old . EOS'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(evaluate_bs('我们的爸爸只有十五岁。', debug=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机选取一个句子进行验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    pair = random.choice(pairs)\n",
    "    \n",
    "    output_words = evaluate(pair[0])\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 生活多么复杂 \n",
      "= life is so complicated .\n",
      "< life is so nice ! EOS\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotbaby/anaconda3/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "life is fun . EOS\n",
      "life is fun . EOS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotbaby/anaconda3/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(evaluate('人生是有趣的。')))\n",
    "print(' '.join(evaluate_bs('人生是有趣的。')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "\n",
    "# 读取测试数据集\n",
    "with open('cn-eng-test.txt') as f:\n",
    "    lines = f.read().strip().split('\\n')\n",
    "    \n",
    "    test_pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "\n",
    "\n",
    "test_pairs_dict = collections.defaultdict(lambda : [])\n",
    "\n",
    "for pair in test_pairs:\n",
    "    test_pairs_dict[pair[0]].append(pair[1].split(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu_score():\n",
    "    candicates = []\n",
    "    references = []\n",
    "\n",
    "    for i, pair in enumerate(test_pairs_dict.items(), start=1):\n",
    "        candicate = evaluate(pair[0])\n",
    "        if candicate[-1] == 'EOS':\n",
    "            candicate.pop(-1)\n",
    "        candicates.append(candicate)\n",
    "        references.append(pair[1])\n",
    "    \n",
    "    score = bleu_score(candicates, references)\n",
    "    return score\n",
    "\n",
    "\n",
    "def evaluate_bs_bleu_score(bs_size=5):\n",
    "    candicates = []\n",
    "    references = []\n",
    "\n",
    "    for i, pair in enumerate(test_pairs_dict.items(), start=1):\n",
    "        candicate = evaluate_bs(pair[0], bs_size=bs_size)\n",
    "        if candicate[-1] == 'EOS':\n",
    "            candicate.pop(-1)\n",
    "        candicates.append(candicate)\n",
    "        references.append(pair[1])\n",
    "    \n",
    "    score = bleu_score(candicates, references)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotbaby/anaconda3/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset bleu score: 0.4404138090273204\n"
     ]
    }
   ],
   "source": [
    "print('test dataset bleu score: %s' % evaluate_bleu_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotbaby/anaconda3/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset beam search bleu score: 0.4699782729148865\n"
     ]
    }
   ],
   "source": [
    "print('test dataset beam search bleu score: %s' % evaluate_bs_bleu_score(bs_size=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用beamsearch预测，BLEU值略有提高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    \"\"\"预测\"\"\"\n",
    "    with open('test.txt') as f:\n",
    "        sentences =  [line.strip() for line in f.readlines()]\n",
    "\n",
    "    output_sentences = []\n",
    "    for sentence in sentences:\n",
    "        output_sentence = ' '.join(evaluate_bs(sentence))\n",
    "        output_sentences.append(output_sentence.strip('EOS'))\n",
    "    \n",
    "    with open('result-bigru.txt', 'w') as f:\n",
    "        f.write('\\n'.join(output_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotbaby/anaconda3/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "predict()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "275.753px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 514.7443639999999,
   "position": {
    "height": "40px",
    "left": "1211.8px",
    "right": "20px",
    "top": "120px",
    "width": "360px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
