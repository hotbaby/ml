{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器翻译评价指标：ELEU(Bilingual Evaluation Understudy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$BLEU = BP * exp(\\prod^{N}\\limits_{i=1} w_i*\\log(p_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $p_i$：i_gram精度\n",
    "* BP：Brevity Penalty，简短惩罚\n",
    "    * $l_c$翻译结果长度，$l_r$参考答案长度\n",
    "    * 1 如果$l_c > l_r$\n",
    "    * $exp(1 - \\frac{l_r}{l_c})$ 如果$l_c \\le l_r$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def split_sentence(sentence, n_gram):\n",
    "    \"\"\"Split sentence.\"\"\"\n",
    "    tokens = []\n",
    "    for i in range(len(sentence)-n_gram+1):\n",
    "        tokens.append(' '.join(sentence[i:i+n_gram]).lower())\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def bleu(prediction: str, ground_truth: str, n_gram: int):\n",
    "    \"\"\"Calculate BiLingual Evaluation Understudy Score.\"\"\"\n",
    "    \n",
    "    prediction = prediction.split(' ')\n",
    "    ground_truth = ground_truth.split(' ')\n",
    "    \n",
    "    # 翻译结果长度\n",
    "    lc = len(prediction)\n",
    "    # 参考答案长度\n",
    "    lr = len(ground_truth)\n",
    "    \n",
    "    # i-gram精度\n",
    "    precisions = []\n",
    "    for i in range(1, n_gram+1):\n",
    "        ground_truth_tokens = split_sentence(ground_truth, i)\n",
    "        predition_tokens = split_sentence(prediction, i)\n",
    "        \n",
    "        precision = sum([True for token in predition_tokens if token in ground_truth_tokens]) / len(predition_tokens)\n",
    "        precisions.append(precision)\n",
    "        \n",
    "    print(precisions)\n",
    "    result = np.exp((1/n_gram) * np.sum([np.log(p) for p in precisions]))\n",
    "    result *= np.exp(1-lr/lc) if lc <= lr else 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = 'Israeli officials are responsible for airport security'\n",
    "prediction = 'Airport security Israeli officials are responsible'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.8, 0.5, 0.3333333333333333]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5115078115793242"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu(prediction, ground_truth, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = '我 在 清 华 搞 自 然 语 言 处 理 。'\n",
    "ground_truth = '我 是 清 华 大 学 自 然 语 言 处 理 实 验 室 的 同 学 。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8333333333333334, 0.5454545454545454, 0.4, 0.3333333333333333]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2768793496359699"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu(prediction, ground_truth, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
