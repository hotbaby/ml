{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Autograd: Automatic Differenttiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Central to all neural networks in PyTorch is the `autograd` package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `autograd` package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single interation can be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "`torch.Tensor` is the central class of the package. If you set its attribute `.requires_grad` as True, it starts to track all operations on it. When you finish you computation you can `.backward()` and have the gradients computed automatically. **The gradient for this tensor will be accumulated into `.grad` attribute.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "To stop a tensor from tracking history, you can call `.detach()` to detach it from the computation history, and to prevent future computation from being tracked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "To prevent tracking history, you can wrap the code block `with torch.no_grad():`. This can be helpful when evaluating a model because the model may have trainable parameters with `requires_grad=True`, but for which we don't need the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**`Tensor` and `Function` are interconnected and build up an acyclic grap, that encodes a complete history of computation.** Each tensor has a `.grad_fn` atrribute that references a `Function` that has created the `Tensor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "If you want to compute the derivatives, you can call `.backward()` on a `Tensor`. If `Tensor` is a scalar, you don't need to specify any arguments to `backward()`, however if it has more elements, you need to specify a `gradient` argument that is a tensor of matching shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Create a tensor and set `requires_grad=True` to track computation with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a tensor operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "`y` was created as a result of an operation, so it has a `grad_fn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7fc278b8f890>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Do more operations on `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "`.reuqires_grad_(...)` changes an existing Tensor's `requires_grad` flag in-place. The input flag defaults to `False` if not given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x7fc278b92110>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Graidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "let's backprop now. Because `out` contains a single scala, `out.backward()` is equivalent to `out.backward(torch.tensor(1.)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Print gradients d(out)/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Mathematically, if you have a vector valued function $\\hat y = f(x)$, then the gradient of $\\hat y$ with respect to $x$ is Jacobian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "![](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Generally speaking, `torch.autograd` is an engine for computing vector-jacobian product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Let's takes a look at an example of vector-Jacobian product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  885.2930, -1133.7174,  1037.9873], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in this case `y` is no longer a scalar. `torch.autograd` could not compute the full jacobian directly, but if we just want the vector-Jacobian product, simply pass the vector to `backward` as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, .0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "You can also stop autograd from tracking history on Tensor with `.requires_grad=True` either by wrapping the code block `with torch.no_grad():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Or by using `.detach()` to get a new Tensor with the same content but that does not require gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
