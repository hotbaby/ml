{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 17. Concurrency with Futures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I introduce the concept of \"futures\" - objects representing the asynchronous execution of an operation. This powerful idea is the foundation not only of `concurrent.futures` but also of the `asyncio` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Web Downloads in Three Styles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle network I/O efficiently, you need concurrency, as it involves high latency so instead of wasting CPU cycles waiting, it's better to do something else until a response come back from the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Sequential Download Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequential download script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "POP20_CC = ('CN IN US ID BR PK NG BD RU JP '\n",
    "            'MX PH VN ET EG DE IR TR CD FR').split()\n",
    "BASE_URL= 'http://flupy.org/data/flags'\n",
    "DEST_DIR = 'downloads/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_flag(img, filename):\n",
    "    path = os.path.join(DEST_DIR, filename)\n",
    "    with open(path, 'wb') as fp:\n",
    "        fp.write(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flag(cc):\n",
    "    url = '{}/{cc}/{cc}.gif'.format(BASE_URL, cc=cc.lower())\n",
    "    print('download %s' % url)\n",
    "    resp = requests.get(url)\n",
    "    return resp.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(text):\n",
    "    print(text, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_many(cc_list):\n",
    "    for cc in cc_list:\n",
    "        image = get_flag(cc)\n",
    "        show(cc)\n",
    "        save_flag(image, cc.lower()+'.gif')\n",
    "        \n",
    "    return len(cc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(download_many):\n",
    "    t0 = time.time()\n",
    "    count = download_many(POP20_CC)\n",
    "    elapsed = time.time() - t0\n",
    "    print('\\n{} flags downloaded in {:.2f}s'.format(count, elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading with concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main features of the `concurrent.futures` package are the ThreadPoolExecutor and ProcessPoolExecutor classes, which implement an interface that allows you submit callables for execution in different threads or processes, respectively. The classes manage an internal pool of worker threads or processes, and a queue of tasks to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORKERS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_one(cc):\n",
    "    image = get_flag(cc)\n",
    "    save_flag(image, cc.lower()+'.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_many(cc_list):\n",
    "    workers = min(MAX_WORKERS, len(cc_list))\n",
    "    # Instantiate the ThreadPoolExecutor with that number of worker threads; the executor.__exit__\n",
    "    # method will call executor.shutdown(wait=True), which will block until all threads\n",
    "    # are done.\n",
    "    with concurrent.futures.ThreadPoolExecutor(workers) as executor:\n",
    "        # The map method is similar to the map in built-in, except that the download_one\n",
    "        # function will be called concurrently from multiple threads; it returns a generator\n",
    "        # that can be iterated over to retrieve the value returned by each function.\n",
    "        res = executor.map(download_one, sorted(cc_list))\n",
    "        \n",
    "    return len(list(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where Are the Futures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Futures are essential components in the internals of concurrent.futures and asyncio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of Python 3.4, there are two classes named Future in the standard library: concurrent.futures.Future and asyncio.Future. They serve the same purpose: **an instance of either Future class represents a deferred computation that may or may not have completed.** This is similar to the Deferred class in Twisted, the Future class in Tornado, and Promise objects in JavaScript libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Futures encapsulate pending operations so that they can be put in queues, their state of completion can be queried, and their results (or exception) can be retrieved when available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An important thing to known is that you and I should nt create them: they are meant to be instantiated exclusively by the concurrency framework.** It's easy to understand why: a Future represents something that will eventually happen, and the only way to be sure that something will happen is to schedule its execution. Therefore, concurrent.futures.Future instances are created by as the result of scheduling something for execution with a concurrent.futures.Executor subclass. For example, the `Executor.submit()` method takes a callable, schedules it to run, and return a future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Client code is not supposed to change the state of a future: the concurrency framework changes the state of a future when the computation it represents is done, and we can't control when that happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Both types of Future have a `.done()` method that is nonblocking and return a Boolean that tells you whether the callable linked to that future has executed or not. Instead of asking whether a future is done, client code usually asks to be noticed. That's why both Future classes have an `.add_done_callback()` method: you give it a callable, and the callable will invoked with the future as the single argument when the future is done.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a `.result()` method, which works the same in both classes when the future is done: it returns the result of the callbale, or re-raises whatever exception might have been throw when the callable was executed. However, when the future is not done, the behavior of the result method is very different between the two flavors of Future. In a concurrency.future.Future instance, invoking `f.result()` will block the caller's thread until the result is ready. An optional timeout argument can be passed, and if the future is not done in the specified time, a `TimeoutError` exception is raised. The `asyncio.Future.result()` method does not support timeout, and the preferred way to get the result of future in that library is to use `yield from` - which doesn't work with `concurrency.future.Future` intances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher-level `executor.map` call is replaced by two for loops: one to create and schedule the futures, the other to retrieve their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_many(cc_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        to_do = []\n",
    "        for cc in sorted(cc_list):\n",
    "            future = executor.submit(download_one, cc)\n",
    "            to_do.append(future)\n",
    "            \n",
    "        results = []\n",
    "        for future in future.as_complete(to_do):\n",
    "            res = future.result()\n",
    "            results.append(res)\n",
    "            \n",
    "    return len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the concurrent scripts we tested can perform downloads in parallel. The `concurrent.futures` examples are limited by the GIL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blocking I/O and the GIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CPython interpreter is not thread-safe internally, so it has Global Interpreter Lock(GIL), which allows only one thread at a time to execute python bytecode. That's why a single Python process usually cannot use multiple CPU cores at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**However, all standard library functions that perform blocking I/O release the GIL when waiting for a result from the OS. This means Python programs that are I/O bound can benefit from using threads at the Python level: while one Python thread is waiting for a response from the network, the blocked I/O function release the GIL, so another thread can run.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Every blocking I/O function in the Python standard library release the GIL,, allowing other thread to run. The `time.sleep()` function also release the GIL. Therefore, Python threads are perfectly usable in I/O-bound applications, despite thee GIL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lauching Processes with concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`concurrent.futures` does enable truly parallel computation because it suppports distirbuting work among multiple Python processes using the `ProcessPoolExecutor` class - thus bypassing the GIL and leveraging all available CPU cores, if you need to do CPU-bound processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace `ThreadPoolExecutor` with `ProcessPoolExecutor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def download_many(cc_list):\n",
    "    with futures.ThreadPoolExecutor() as executor:\n",
    "        pass\n",
    "\n",
    "To \n",
    "\n",
    "def download_many(cc_list):\n",
    "    with futures.ProcessPoolExecutor() as executor:\n",
    "        pass\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Executor.map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to run several callables concurrently is with the Executor.map function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:52:22] Script starting\n",
      "[09:52:22] loiter(0): doing nothing for 0s...\n",
      "[09:52:22] loiter(0): done.\n",
      "[09:52:22] \tloiter(1): doing nothing for 1s...[09:52:22]\n",
      "[09:52:22][09:52:22] results: <generator object Executor.map.<locals>.result_iterator at 0x10f3087c8>\n",
      "[09:52:22] result 0: 0\n",
      " \t\tloiter(2): doing nothing for 2s...\n",
      " \t\t\tloiter(3): doing nothing for 3s...\n",
      "[09:52:23] \tloiter(1): done.\n",
      "[09:52:23] \t\t\t\tloiter(4): doing nothing for 4s...\n",
      "[09:52:23] result 1: 10\n",
      "[09:52:24] \t\tloiter(2): done.\n",
      "[09:52:24] result 2: 20\n",
      "[09:52:25] \t\t\tloiter(3): done.\n",
      "[09:52:25] result 3: 30\n",
      "[09:52:27] \t\t\t\tloiter(4): done.\n",
      "[09:52:27] result 4: 40\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import concurrent\n",
    "\n",
    "def display(*args):\n",
    "    print(time.strftime('[%H:%M:%S]'), end=' ')\n",
    "    print(*args)\n",
    "\n",
    "def loiter(n):\n",
    "    msg = '{}loiter({}): doing nothing for {}s...'\n",
    "    display(msg.format('\\t'*n, n, n))\n",
    "    time.sleep(n)\n",
    "    msg = '{}loiter({}): done.'\n",
    "    display(msg.format('\\t'*n, n))\n",
    "    return n * 10\n",
    "\n",
    "def main():\n",
    "    display('Script starting')\n",
    "    executor = concurrent.futures.ThreadPoolExecutor(max_workers=3)\n",
    "    results = executor.map(loiter, range(5))\n",
    "    display('results:', results)\n",
    "    for i, result in enumerate(results):\n",
    "        display('result {}: {}'.format(i, result))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `Executor.map` function is easy to use but it has feature that may or may not be helpful, depending on your needs: it returns the result exactly in the same order as the calls are started: if the first call take 10s to produce a result, and the other takes 1s each, you code will block for 10s as it tries to retrieve the first result of the generator returned by map.** After that, you'll get the remaining results without blocking because they will be done. That's OK when you must have all the results before proceeding, but often it's preferabel get the results as they are ready, regardless of the order they were submitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The combination of `executor.submit` and `futures.as_completed` is more flexible than `executor.map` because you can submit different callables and arguments, while `executor.map` is designed to run the same callable on the different arguments. In addition, the set of futures you pass to `futures.as_completed` may came from different more than one executor - perhaps some were created by a ThreadPoolExecutor instance while others are from a `ProcessPoolExecutor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
