{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP From Scratch: Generating Names With A Character-Level RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# categories: 18 ['Czech', 'German', 'Arabic', 'Japanese', 'Chinese', 'Vietnamese', 'Russian', 'French', 'Irish', 'English', 'Spanish', 'Greek', 'Italian', 'Portuguese', 'Scottish', 'Dutch', 'Korean', 'Polish']\n",
      "O'Neal\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "n_letters = len(all_letters) + 1 # Plus EOS marker\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "# Build the category_lines dictionary, a list of lines per category\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "if n_categories == 0:\n",
    "    raise RuntimeError('Data not found. Make sure that you downloaded data '\n",
    "        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n",
    "        'the current directory.')\n",
    "\n",
    "print('# categories:', n_categories, all_categories)\n",
    "print(unicodeToAscii(\"O'Néàl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18,\n",
       " ['Czech',\n",
       "  'German',\n",
       "  'Arabic',\n",
       "  'Japanese',\n",
       "  'Chinese',\n",
       "  'Vietnamese',\n",
       "  'Russian',\n",
       "  'French',\n",
       "  'Irish',\n",
       "  'English',\n",
       "  'Spanish',\n",
       "  'Greek',\n",
       "  'Italian',\n",
       "  'Portuguese',\n",
       "  'Scottish',\n",
       "  'Dutch',\n",
       "  'Korean',\n",
       "  'Polish'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_categories, all_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 'Czech')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(category_lines), all_categories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abl',\n",
       " 'Adsit',\n",
       " 'Ajdrna',\n",
       " 'Alt',\n",
       " 'Antonowitsch',\n",
       " 'Antonowitz',\n",
       " 'Bacon',\n",
       " 'Ballalatak',\n",
       " 'Ballaltick',\n",
       " 'Bartonova',\n",
       " 'Bastl',\n",
       " 'Baroch',\n",
       " 'Benesch',\n",
       " 'Betlach',\n",
       " 'Biganska',\n",
       " 'Bilek',\n",
       " 'Blahut',\n",
       " 'Blazek',\n",
       " 'Blazek',\n",
       " 'Blazejovsky',\n",
       " 'Blecha',\n",
       " 'Bleskan',\n",
       " 'Blober',\n",
       " 'Bock',\n",
       " 'Bohac',\n",
       " 'Bohunovsky',\n",
       " 'Bolcar',\n",
       " 'Borovka',\n",
       " 'Borovski',\n",
       " 'Borowski',\n",
       " 'Borovsky',\n",
       " 'Brabbery',\n",
       " 'Brezovjak',\n",
       " 'Brousil',\n",
       " 'Bruckner',\n",
       " 'Buchta',\n",
       " 'Cablikova',\n",
       " 'Camfrlova',\n",
       " 'Cap',\n",
       " 'Cerda',\n",
       " 'Cermak',\n",
       " 'Chermak',\n",
       " 'Cermak',\n",
       " 'Cernochova',\n",
       " 'Cernohous',\n",
       " 'Cerny',\n",
       " 'Cerney',\n",
       " 'Cerny',\n",
       " 'Cerv',\n",
       " 'Cervenka',\n",
       " 'Chalupka',\n",
       " 'Charlott',\n",
       " 'Chemlik',\n",
       " 'Chicken',\n",
       " 'Chilar',\n",
       " 'Chromy',\n",
       " 'Cihak',\n",
       " 'Clineburg',\n",
       " 'Klineberg',\n",
       " 'Cober',\n",
       " 'Colling',\n",
       " 'Cvacek',\n",
       " 'Czabal',\n",
       " 'Damell',\n",
       " 'Demall',\n",
       " 'Dehmel',\n",
       " 'Dana',\n",
       " 'Dejmal',\n",
       " 'Dempko',\n",
       " 'Demko',\n",
       " 'Dinko',\n",
       " 'Divoky',\n",
       " 'Dolejsi',\n",
       " 'Dolezal',\n",
       " 'Doljs',\n",
       " 'Dopita',\n",
       " 'Drassal',\n",
       " 'Driml',\n",
       " 'Duyava',\n",
       " 'Dvorak',\n",
       " 'Dziadik',\n",
       " 'Egr',\n",
       " 'Entler',\n",
       " 'Faltysek',\n",
       " 'Faltejsek',\n",
       " 'Fencl',\n",
       " 'Fenyo',\n",
       " 'Fillipova',\n",
       " 'Finfera',\n",
       " 'Finferovy',\n",
       " 'Finke',\n",
       " 'Fojtikova',\n",
       " 'Fremut',\n",
       " 'Friedrich',\n",
       " 'Frierdich',\n",
       " 'Fritsch',\n",
       " 'Furtsch',\n",
       " 'Gabrisova',\n",
       " 'Gavalok',\n",
       " 'Geier',\n",
       " 'Georgijev',\n",
       " 'Geryk',\n",
       " 'Giersig',\n",
       " 'Glatter',\n",
       " 'Glockl',\n",
       " 'Grabski',\n",
       " 'Grozmanova',\n",
       " 'Grulich',\n",
       " 'Grygarova',\n",
       " 'Hadash',\n",
       " 'Hafernik',\n",
       " 'Hajek',\n",
       " 'Hajicek',\n",
       " 'Hajkova',\n",
       " 'Hana',\n",
       " 'Hanek',\n",
       " 'Hanek',\n",
       " 'Hanika',\n",
       " 'Hanusch',\n",
       " 'Hanzlick',\n",
       " 'Handzlik',\n",
       " 'Hanzlik',\n",
       " 'Harger',\n",
       " 'Hartl',\n",
       " 'Havlatova',\n",
       " 'Havlice',\n",
       " 'Hawlata',\n",
       " 'Heidl',\n",
       " 'Herback',\n",
       " 'Herodes',\n",
       " 'Hiorvst',\n",
       " 'Hladky',\n",
       " 'Hlavsa',\n",
       " 'Hnizdil',\n",
       " 'Hodowal',\n",
       " 'Hodoval',\n",
       " 'Holan',\n",
       " 'Holub',\n",
       " 'Homulka',\n",
       " 'Hora',\n",
       " 'Hovanec',\n",
       " 'Hrabak',\n",
       " 'Hradek',\n",
       " 'Hrdy',\n",
       " 'Hrula',\n",
       " 'Hruska',\n",
       " 'Hruskova',\n",
       " 'Hudecek',\n",
       " 'Husk',\n",
       " 'Hynna',\n",
       " 'Jaluvka',\n",
       " 'Janca',\n",
       " 'Janicek',\n",
       " 'Jenicek',\n",
       " 'Janacek',\n",
       " 'Janick',\n",
       " 'Janoch',\n",
       " 'Janosik',\n",
       " 'Janutka',\n",
       " 'Jares',\n",
       " 'Jarzembowski',\n",
       " 'Jedlicka',\n",
       " 'Jelinek',\n",
       " 'Jindra',\n",
       " 'Jirava',\n",
       " 'Jirik',\n",
       " 'Jirku',\n",
       " 'Jirovy',\n",
       " 'Jobst',\n",
       " 'Jonas',\n",
       " 'Kacirek',\n",
       " 'Kafka',\n",
       " 'Kafka',\n",
       " 'Kaiser',\n",
       " 'Kanak',\n",
       " 'Kaplanek',\n",
       " 'Kara',\n",
       " 'Karlovsky',\n",
       " 'Kasa',\n",
       " 'Kasimor',\n",
       " 'Kazimor',\n",
       " 'Kazmier',\n",
       " 'Katschker',\n",
       " 'Kauphsman',\n",
       " 'Kenzel',\n",
       " 'Kerner',\n",
       " 'Kesl',\n",
       " 'Kessel',\n",
       " 'Kessler',\n",
       " 'Khork',\n",
       " 'Kirchma',\n",
       " 'Klein',\n",
       " 'Klemper',\n",
       " 'Klimes',\n",
       " 'Kober',\n",
       " 'Koberna',\n",
       " 'Koci',\n",
       " 'Kocian',\n",
       " 'Kocian',\n",
       " 'Kofron',\n",
       " 'Kolacny',\n",
       " 'Koliha',\n",
       " 'Kolman',\n",
       " 'Koma',\n",
       " 'Komo',\n",
       " 'Coma',\n",
       " 'Konarik',\n",
       " 'Kopp',\n",
       " 'Kopecky',\n",
       " 'Korandak',\n",
       " 'Korycan',\n",
       " 'Korycansky',\n",
       " 'Kosko',\n",
       " 'Kouba',\n",
       " 'Kouba',\n",
       " 'Koukal',\n",
       " 'Koza',\n",
       " 'Kozumplikova',\n",
       " 'Kratschmar',\n",
       " 'Krawiec',\n",
       " 'Kreisinger',\n",
       " 'Kremlacek',\n",
       " 'Kremlicka',\n",
       " 'Kreutschmer',\n",
       " 'Krhovsky',\n",
       " 'Krivan',\n",
       " 'Krivolavy',\n",
       " 'Kriz',\n",
       " 'Kruessel',\n",
       " 'Krupala',\n",
       " 'Krytinar',\n",
       " 'Kubin',\n",
       " 'Kucera',\n",
       " 'Kucharova',\n",
       " 'Kudrna',\n",
       " 'Kuffel',\n",
       " 'Kupfel',\n",
       " 'Kofel',\n",
       " 'Kulhanek',\n",
       " 'Kunik',\n",
       " 'Kurtz',\n",
       " 'Kusak',\n",
       " 'Kvasnicka',\n",
       " 'Lawa',\n",
       " 'Linart',\n",
       " 'Lind',\n",
       " 'Lokay',\n",
       " 'Loskot',\n",
       " 'Ludwig',\n",
       " 'Lynsmeier',\n",
       " 'Macha',\n",
       " 'Machacek',\n",
       " 'Macikova',\n",
       " 'Malafa',\n",
       " 'Malec',\n",
       " 'Malecha',\n",
       " 'Maly',\n",
       " 'Marek',\n",
       " 'Marik',\n",
       " 'Marik',\n",
       " 'Markytan',\n",
       " 'Matejka',\n",
       " 'Matjeka',\n",
       " 'Matocha',\n",
       " 'MaxaB',\n",
       " 'Mayer',\n",
       " 'Meier',\n",
       " 'Merta',\n",
       " 'Meszes',\n",
       " 'Metjeka',\n",
       " 'Michalovic',\n",
       " 'Michalovicova',\n",
       " 'Miksatkova',\n",
       " 'Mojzis',\n",
       " 'Mojjis',\n",
       " 'Mozzis',\n",
       " 'Molcan',\n",
       " 'Monfort',\n",
       " 'MonkoAustria',\n",
       " 'Morava',\n",
       " 'Morek',\n",
       " 'Muchalon',\n",
       " 'Mudra',\n",
       " 'Muhlbauer',\n",
       " 'Nadvornizch',\n",
       " 'Nadwornik',\n",
       " 'Navara',\n",
       " 'Navratil',\n",
       " 'Navratil',\n",
       " 'Navrkal',\n",
       " 'Nekuza',\n",
       " 'Nemec',\n",
       " 'Nemecek',\n",
       " 'Nestrojil',\n",
       " 'Netsch',\n",
       " 'Neusser',\n",
       " 'Neisser',\n",
       " 'Naizer',\n",
       " 'Novak',\n",
       " 'Nowak',\n",
       " 'Novotny',\n",
       " 'Novy Novy',\n",
       " 'Oborny',\n",
       " 'Ocasek',\n",
       " 'Ocaskova',\n",
       " 'Oesterreicher',\n",
       " 'Okenfuss',\n",
       " 'Olbrich',\n",
       " 'Ondrisek',\n",
       " 'Opizka',\n",
       " 'Opova',\n",
       " 'Opp',\n",
       " 'Osladil',\n",
       " 'Ozimuk',\n",
       " 'Pachr',\n",
       " 'Palzewicz',\n",
       " 'Panek',\n",
       " 'Patril',\n",
       " 'Pavlik',\n",
       " 'Pavlicka',\n",
       " 'Pavlu',\n",
       " 'Pawlak',\n",
       " 'Pear',\n",
       " 'Peary',\n",
       " 'Pech',\n",
       " 'Peisar',\n",
       " 'Paisar',\n",
       " 'Paiser',\n",
       " 'Perevuznik',\n",
       " 'Perina',\n",
       " 'Persein',\n",
       " 'Petrezelka',\n",
       " 'Petru',\n",
       " 'Pesek',\n",
       " 'Petersen',\n",
       " 'Pfeifer',\n",
       " 'Picha',\n",
       " 'Pillar',\n",
       " 'Pellar',\n",
       " 'Piller',\n",
       " 'Pinter',\n",
       " 'Pitterman',\n",
       " 'Planick',\n",
       " 'Piskach',\n",
       " 'Plisek',\n",
       " 'Plisko',\n",
       " 'Pokorny',\n",
       " 'Ponec',\n",
       " 'Ponec',\n",
       " 'Prachar',\n",
       " 'Praseta',\n",
       " 'Prchal',\n",
       " 'Prehatney',\n",
       " 'Pretsch',\n",
       " 'Prill',\n",
       " 'Psik',\n",
       " 'Pudel',\n",
       " 'Purdes',\n",
       " 'Quasninsky',\n",
       " 'Raffel',\n",
       " 'Rafaj',\n",
       " 'Ransom',\n",
       " 'Rezac',\n",
       " 'Riedel',\n",
       " 'Riha',\n",
       " 'Riha',\n",
       " 'Ritchie',\n",
       " 'Rozinek',\n",
       " 'Ruba',\n",
       " 'Ruda',\n",
       " 'Rumisek',\n",
       " 'Ruzicka',\n",
       " 'Rypka',\n",
       " 'Rebka',\n",
       " 'Rzehak',\n",
       " 'Sabol',\n",
       " 'Safko',\n",
       " 'Samz',\n",
       " 'Sankovsky',\n",
       " 'Sappe',\n",
       " 'Sappe',\n",
       " 'Sarna',\n",
       " 'Satorie',\n",
       " 'Savchak',\n",
       " 'Svotak',\n",
       " 'Swatchak',\n",
       " 'Svocak',\n",
       " 'Svotchak',\n",
       " 'Schallom',\n",
       " 'Schenk',\n",
       " 'Schlantz',\n",
       " 'Schmeiser',\n",
       " 'Schneider',\n",
       " 'Schmied',\n",
       " 'Schubert',\n",
       " 'Schwarz',\n",
       " 'Schwartz',\n",
       " 'Sedmik',\n",
       " 'Sedmikova',\n",
       " 'Seger',\n",
       " 'Sekovora',\n",
       " 'Semick',\n",
       " 'Serak',\n",
       " 'Sherak',\n",
       " 'Shima',\n",
       " 'Shula',\n",
       " 'Siegl',\n",
       " 'Silhan',\n",
       " 'Simecek',\n",
       " 'Simodines',\n",
       " 'Simonek',\n",
       " 'Sip',\n",
       " 'Sitta',\n",
       " 'Skala',\n",
       " 'Skeril',\n",
       " 'Skokan',\n",
       " 'Skomicka',\n",
       " 'Skwor',\n",
       " 'Slapnickova',\n",
       " 'Slejtr',\n",
       " 'Slepicka',\n",
       " 'Slepica',\n",
       " 'Slezak',\n",
       " 'Slivka',\n",
       " 'Smith',\n",
       " 'Snelker',\n",
       " 'Sokolik',\n",
       " 'Soucek',\n",
       " 'Soukup',\n",
       " 'Soukup',\n",
       " 'Spicka',\n",
       " 'Spoerl',\n",
       " 'Sponer',\n",
       " 'Srda',\n",
       " 'Srpcikova',\n",
       " 'Stangl',\n",
       " 'Stanzel',\n",
       " 'Stary',\n",
       " 'Staska',\n",
       " 'Stedronsky',\n",
       " 'Stegon',\n",
       " 'Sztegon',\n",
       " 'Steinborn',\n",
       " 'Stepan',\n",
       " 'Stites',\n",
       " 'Stluka',\n",
       " 'Stotzky',\n",
       " 'StrakaO',\n",
       " 'Stramba',\n",
       " 'Stupka',\n",
       " 'Subertova',\n",
       " 'Suchanka',\n",
       " 'Sula',\n",
       " 'Svejda',\n",
       " 'Svejkovsky',\n",
       " 'Svoboda',\n",
       " 'Tejc',\n",
       " 'Tikal',\n",
       " 'Tykal',\n",
       " 'Till',\n",
       " 'Timpe',\n",
       " 'Timpy',\n",
       " 'Toman',\n",
       " 'Tomanek',\n",
       " 'Tomasek',\n",
       " 'Tomes',\n",
       " 'Trampotova',\n",
       " 'Trampota',\n",
       " 'Treblik',\n",
       " 'Trnkova',\n",
       " 'Uerling',\n",
       " 'Uhlik',\n",
       " 'Urbanek',\n",
       " 'Urbanek',\n",
       " 'Urbanovska',\n",
       " 'Urista',\n",
       " 'Ustohal',\n",
       " 'Vaca',\n",
       " 'Vaculova',\n",
       " 'Vavra',\n",
       " 'Vejvoda',\n",
       " 'Veverka',\n",
       " 'Victor',\n",
       " 'Vlach',\n",
       " 'Vlach',\n",
       " 'Vlasak',\n",
       " 'Vlasek',\n",
       " 'Volcik',\n",
       " 'Voneve',\n",
       " 'Votke',\n",
       " 'Vozab',\n",
       " 'Vrazel',\n",
       " 'Vykruta',\n",
       " 'Wykruta',\n",
       " 'Waclauska',\n",
       " 'Weichert',\n",
       " 'Weineltk',\n",
       " 'Weisener',\n",
       " 'Wiesner',\n",
       " 'Wizner',\n",
       " 'Weiss',\n",
       " 'Werlla',\n",
       " 'Whitmire',\n",
       " 'Widerlechner',\n",
       " 'Wilchek',\n",
       " 'Wondracek',\n",
       " 'Wood',\n",
       " 'Zajicek',\n",
       " 'Zak',\n",
       " 'Zajicek',\n",
       " 'Zaruba',\n",
       " 'Zaruba',\n",
       " 'Zelinka',\n",
       " 'Zeman',\n",
       " 'Zimola',\n",
       " 'Zipperer',\n",
       " 'Zitka',\n",
       " 'Zoucha',\n",
       " 'Zwolenksy']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_lines[all_categories[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network extends the character-level rnn classification with an extra argument for the category tensor, which is concatenated along with the others. The category tensor is a one-hot vector just like the letter input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will interpret the output as the probability of the next letter. When sampling, the most likely output letter is used as the next input letter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added a second linear layer `o2o`(after combining hidden and output) to given it must muscle to work with. There's also a dropout layer, which randomly zeros parts of its input with a givene probability and is usually used to fuzz inputs to prevent overfitting. Here we're using it towards the end of the network to purposely add some chaos and increase sampling variety."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/jzVrf7f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, category, input, hidden):\n",
    "        input_combined = torch.cat((category, input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        output_combined = torch.cat((hidden, output), 1)\n",
    "        output = self.o2o(output_combined)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir='runs/rnn_generation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l)-1)]\n",
    "\n",
    "\n",
    "def randomTrainingPair():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    return category, line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each timestep the inputs of the network will be `(category, currnet letter, hidden state)` and the outputs will be `(next letter, next hidden state)`. So for each training set, we'll need the category, a set of input letters, and a set of output/target letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/JH58tXY.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 18)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_categories), n_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The category tensor is a one-hot tensor of size `<1 * n_categories>`. When training we feed it to the network at every timestep - this is a design choice, it could have been includedw as part of initial hidden state or some other strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot vector for category\n",
    "def categoryTensor(category):\n",
    "    li = all_categories.index(category)\n",
    "    tensor = torch.zeros(1, n_categories)\n",
    "    tensor[0][li] = 1\n",
    "    return tensor\n",
    "\n",
    "# One-hot matrix of first to last letters (not including EOS) for input\n",
    "def inputTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "        tensor[li][0][all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# LongTensor of second letter to end (EOS) for target\n",
    "def targetTensor(line):\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience during training we'll make a `randomTrainExample` function that fetcheh a random (category, line) pair and turns them into the required (category, input, target) tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make category, input, and target tensors from a random category, line pair\n",
    "def randomTrainingExample():\n",
    "    category, line = randomTrainingPair()\n",
    "    category_tensor = categoryTensor(category)\n",
    "    input_line_tensor = inputTensor(line)\n",
    "    target_line_tensor = targetTensor(line)\n",
    "    return category_tensor, input_line_tensor, target_line_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**visualizating computiation graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the computiation graph\n",
    "\n",
    "category_tensor, input_line_tensor, _ = randomTrainingExample()\n",
    "\n",
    "rnn = RNN(n_letters, 128, n_letters)\n",
    "input_tensor = input_line_tensor[0]\n",
    "hidden = rnn.initHidden()\n",
    "\n",
    "writer.add_graph(rnn, (category_tensor, input_tensor, hidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In constrast to classification, where only the last output is used, we are making a prediction at every step, so we can calculating loss at every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magic of autograd allows you to simply sum these losses at each step and call backward at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate = 5e-4\n",
    "\n",
    "def train(category_tensor, input_line_tensor, target_line_tensor):\n",
    "    target_line_tensor.unsqueeze_(-1)\n",
    "    hidden = rnn.initHidden()\n",
    "    \n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(input_line_tensor.size(0)):\n",
    "        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
    "        l = criterion(output, target_line_tensor[i])\n",
    "        loss += l\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "        \n",
    "    return output, loss.item() / input_line_tensor.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep track of how training takes I am adding a `timeSince(timestamp)` function which returns a hunman readable string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is business as usual - call train a bunch of times and wait a few minutes, printing the current time and loss every `print_every` examples, and keeping store of an average loss per `plot_every` examples in `all_losses` for plotting later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(n_letters, 128, n_letters)\n",
    "\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters+1):\n",
    "    output, loss = train(*randomTrainingExample())\n",
    "    total_loss += loss\n",
    "    \n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "        \n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        totoal_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15524ca90>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample we give the network a letter and ask what the next one is, feed that is as the next letter, and repeat until the EOS token.\n",
    "\n",
    "* Crate tensors for input category, starting letter, and empty hidden state\n",
    "* Create a string `output_name` with the starting letter\n",
    "* Up to maximum output length,\n",
    "    * Feed the current letter to the network\n",
    "    * Get the next letter from highest output, and next hidden state\n",
    "    * If the letter is EOS, stop here\n",
    "    * If the regular letter, add to output and continue\n",
    "* Return the final name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Rather than having to give it a starting letter, another strategy would have been to include a \"start of string\" token in training and have the network choose its own starting letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 20\n",
    "\n",
    "# Sample form a category and starting letter\n",
    "\n",
    "def sample(category, start_letter='A'):\n",
    "    with torch.no_grad():\n",
    "        category_tensor = categoryTensor(category)\n",
    "        input = inputTensor(start_letter)\n",
    "        hidden = rnn.initHidden()\n",
    "        \n",
    "        output_name = start_letter\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            output, hidden = rnn(category_tensor, input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = all_letters[topi]\n",
    "                output_name += letter\n",
    "            input = inputTensor(letter)\n",
    "            \n",
    "        return output_name\n",
    "    \n",
    "def sampels(category, start_letters='ABC'):\n",
    "    for start_letter in start_letters:\n",
    "        print(sample(category, start_letter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
