{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /Users/hotbaby/github/firstcell.py\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import tqdm\n",
    "import math\n",
    "import time\n",
    "import heapq\n",
    "import datetime\n",
    "import itertools\n",
    "import functools\n",
    "import collections\n",
    "import multiprocessing\n",
    "\n",
    "import sklearn\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cbt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Pytorch with Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it core, Pytorch provides two main fetures:\n",
    "\n",
    "* An n-dimensional Tensor, similar to numpy but can run on GPUs\n",
    "* Automatic differentiation for building and training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; It does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a two-layer network to random data by manually implementing the forward and backward pases through the network using numpy operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33447692.586147226\n",
      "1 34401835.8170425\n",
      "2 38428616.47655048\n",
      "3 38227918.955900975\n",
      "4 29672959.254181065\n",
      "5 17291590.956716552\n",
      "6 8175109.097472004\n",
      "7 3760339.252569851\n",
      "8 1988641.218177071\n",
      "9 1273825.6597851687\n",
      "10 940588.8007317379\n",
      "11 750712.2540707194\n",
      "12 622505.2865585957\n",
      "13 526387.0934218788\n",
      "14 450404.42960885365\n",
      "15 388542.98185050755\n",
      "16 337374.24275714747\n",
      "17 294699.47753952583\n",
      "18 258778.5898167928\n",
      "19 228270.625767621\n",
      "20 202212.76136464305\n",
      "21 179823.79186596852\n",
      "22 160481.1422227469\n",
      "23 143703.9444104074\n",
      "24 129133.17304258805\n",
      "25 116395.56418243691\n",
      "26 105200.44745841174\n",
      "27 95318.59316503195\n",
      "28 86566.68319539854\n",
      "29 78796.58486337948\n",
      "30 71870.08204446886\n",
      "31 65683.47066779397\n",
      "32 60145.64937301133\n",
      "33 55169.47287126069\n",
      "34 50687.88510976778\n",
      "35 46641.57081860729\n",
      "36 42982.759276475364\n",
      "37 39666.685886719635\n",
      "38 36656.148052121745\n",
      "39 33917.24932860592\n",
      "40 31420.6503024494\n",
      "41 29143.027723317537\n",
      "42 27060.445702065786\n",
      "43 25152.970788030158\n",
      "44 23403.772594415906\n",
      "45 21797.13384930251\n",
      "46 20319.841729301887\n",
      "47 18961.56469641274\n",
      "48 17709.68295993118\n",
      "49 16553.661251308135\n",
      "50 15485.500974165652\n",
      "51 14497.033149694507\n",
      "52 13581.514276148679\n",
      "53 12732.801549101256\n",
      "54 11945.446486280794\n",
      "55 11214.10071554039\n",
      "56 10534.076498177037\n",
      "57 9901.450928800949\n",
      "58 9312.099614211853\n",
      "59 8762.814971868225\n",
      "60 8250.42805603688\n",
      "61 7772.254670021681\n",
      "62 7325.663837097948\n",
      "63 6908.365123344747\n",
      "64 6517.954318425961\n",
      "65 6152.554498754604\n",
      "66 5810.08474430461\n",
      "67 5489.236885357805\n",
      "68 5188.236491451944\n",
      "69 4905.749934345826\n",
      "70 4640.528158587176\n",
      "71 4391.2988793409295\n",
      "72 4157.090478811567\n",
      "73 3936.728017422769\n",
      "74 3729.3719497405523\n",
      "75 3534.1363907512127\n",
      "76 3350.2920799715866\n",
      "77 3176.954164876469\n",
      "78 3013.5300130538526\n",
      "79 2859.412698039748\n",
      "80 2713.928221352059\n",
      "81 2576.6048024824786\n",
      "82 2446.9265401948987\n",
      "83 2324.4134570560436\n",
      "84 2208.580940963411\n",
      "85 2099.0813502848723\n",
      "86 1995.5067375625435\n",
      "87 1897.4913801067146\n",
      "88 1804.7711843406819\n",
      "89 1716.9801342114156\n",
      "90 1633.7859649124575\n",
      "91 1555.0116995516491\n",
      "92 1480.3685982883676\n",
      "93 1409.589359010603\n",
      "94 1342.4580095493598\n",
      "95 1278.8165693127166\n",
      "96 1218.4046570410196\n",
      "97 1161.0614549651\n",
      "98 1106.6420557229976\n",
      "99 1055.0395354645152\n",
      "100 1006.0211244684313\n",
      "101 959.4693047424992\n",
      "102 915.2163888472068\n",
      "103 873.1540905194095\n",
      "104 833.1687951096746\n",
      "105 795.1305006028795\n",
      "106 759.0036658790067\n",
      "107 724.6968670530359\n",
      "108 692.0389768856747\n",
      "109 660.955655227762\n",
      "110 631.364310577919\n",
      "111 603.1772402756958\n",
      "112 576.3269381201269\n",
      "113 550.7583911757818\n",
      "114 526.386355480221\n",
      "115 503.1624088559604\n",
      "116 481.0291390169259\n",
      "117 459.9283653663582\n",
      "118 439.80235416187406\n",
      "119 420.61242054950674\n",
      "120 402.30069198354477\n",
      "121 384.83494423154355\n",
      "122 368.1720432353475\n",
      "123 352.2675769661425\n",
      "124 337.08731950312125\n",
      "125 322.59992339216376\n",
      "126 308.76287016725666\n",
      "127 295.5536105206046\n",
      "128 282.93824651392777\n",
      "129 270.88853750784324\n",
      "130 259.3779020408655\n",
      "131 248.37911740953996\n",
      "132 237.86869171797258\n",
      "133 227.82752833787475\n",
      "134 218.22919869975024\n",
      "135 209.055862912177\n",
      "136 200.28613568440358\n",
      "137 191.8995776065646\n",
      "138 183.88083455514692\n",
      "139 176.21396933160526\n",
      "140 168.8804703965068\n",
      "141 161.86397784786453\n",
      "142 155.15342739029757\n",
      "143 148.73131635091977\n",
      "144 142.58764394857621\n",
      "145 136.70930763152592\n",
      "146 131.0823751640301\n",
      "147 125.69696497678818\n",
      "148 120.5422455363175\n",
      "149 115.60707232171517\n",
      "150 110.8834677485672\n",
      "151 106.35919742157692\n",
      "152 102.02698369056952\n",
      "153 97.87866370903066\n",
      "154 93.90556108203069\n",
      "155 90.09934115813434\n",
      "156 86.45422162330618\n",
      "157 82.9612712449226\n",
      "158 79.61520685994948\n",
      "159 76.40913486427772\n",
      "160 73.33849279999649\n",
      "161 70.39727657757028\n",
      "162 67.57753532425453\n",
      "163 64.87502910678589\n",
      "164 62.28491169730425\n",
      "165 59.801327387325415\n",
      "166 57.4210309842406\n",
      "167 55.139279654847\n",
      "168 52.95019352461047\n",
      "169 50.851161054272694\n",
      "170 48.83878710284695\n",
      "171 46.9086915004813\n",
      "172 45.05714368581536\n",
      "173 43.28119945328997\n",
      "174 41.577611568076264\n",
      "175 39.94308099298794\n",
      "176 38.37511332425322\n",
      "177 36.870785739643246\n",
      "178 35.42731909668406\n",
      "179 34.04222860334691\n",
      "180 32.71297932117446\n",
      "181 31.4374615612995\n",
      "182 30.213053353472258\n",
      "183 29.037819702777952\n",
      "184 27.910016655241286\n",
      "185 26.82720201979209\n",
      "186 25.78778589364496\n",
      "187 24.78977286188876\n",
      "188 23.831459245850013\n",
      "189 22.911297105823024\n",
      "190 22.02788670470074\n",
      "191 21.179400574232766\n",
      "192 20.364621001206316\n",
      "193 19.582209469850383\n",
      "194 18.830636008315317\n",
      "195 18.108782284146756\n",
      "196 17.415531178869152\n",
      "197 16.74945796483877\n",
      "198 16.109578369563174\n",
      "199 15.494940494894145\n",
      "200 14.904365928995222\n",
      "201 14.336949740530166\n",
      "202 13.791767960307386\n",
      "203 13.267896982856682\n",
      "204 12.764461344448804\n",
      "205 12.280667809212305\n",
      "206 11.815694498897313\n",
      "207 11.368766277553217\n",
      "208 10.939273022407143\n",
      "209 10.526391738738878\n",
      "210 10.129507487224338\n",
      "211 9.748030990378275\n",
      "212 9.381345000895006\n",
      "213 9.028789366015543\n",
      "214 8.689875390426916\n",
      "215 8.364036163039769\n",
      "216 8.050678669403375\n",
      "217 7.749335811260452\n",
      "218 7.459603573222476\n",
      "219 7.180979094804167\n",
      "220 6.913011259994099\n",
      "221 6.655358131474812\n",
      "222 6.407546769579117\n",
      "223 6.169183280588939\n",
      "224 5.939895380358204\n",
      "225 5.71933980499009\n",
      "226 5.507237418910814\n",
      "227 5.303199759759538\n",
      "228 5.10684835049747\n",
      "229 4.9179488697825935\n",
      "230 4.736256150618916\n",
      "231 4.561447554523551\n",
      "232 4.393223944899397\n",
      "233 4.231354119394288\n",
      "234 4.07562077957359\n",
      "235 3.9257391042621927\n",
      "236 3.7814986927375784\n",
      "237 3.6426959300595865\n",
      "238 3.5091008601986027\n",
      "239 3.3805590168983297\n",
      "240 3.25683395804258\n",
      "241 3.1377387779055743\n",
      "242 3.0230912328449886\n",
      "243 2.9127182267129514\n",
      "244 2.806491649605468\n",
      "245 2.7042276719763754\n",
      "246 2.6057829710324016\n",
      "247 2.511003218643203\n",
      "248 2.419748125917908\n",
      "249 2.3318829154723733\n",
      "250 2.2472885642693754\n",
      "251 2.1658532274680957\n",
      "252 2.0874115376338764\n",
      "253 2.0118768842299577\n",
      "254 1.9391540954568967\n",
      "255 1.8691227125529835\n",
      "256 1.8016596931732902\n",
      "257 1.7366936474874217\n",
      "258 1.6741429464284707\n",
      "259 1.6138766612991955\n",
      "260 1.5558301405046056\n",
      "261 1.4999169180940775\n",
      "262 1.4460791842113192\n",
      "263 1.3942085338807386\n",
      "264 1.3442268448157457\n",
      "265 1.2960828077318256\n",
      "266 1.2496988724386076\n",
      "267 1.2050136731383307\n",
      "268 1.1619620609015902\n",
      "269 1.1204916334677555\n",
      "270 1.0805262162251263\n",
      "271 1.0420169887248565\n",
      "272 1.0049126124675365\n",
      "273 0.969160132782785\n",
      "274 0.9347056428565055\n",
      "275 0.9015016847942712\n",
      "276 0.8695082383855338\n",
      "277 0.8386698505023598\n",
      "278 0.8089462928785016\n",
      "279 0.7803058500795399\n",
      "280 0.7526985188345008\n",
      "281 0.726088195408471\n",
      "282 0.7004401883510888\n",
      "283 0.6757163974319313\n",
      "284 0.651889223235413\n",
      "285 0.6289138217175405\n",
      "286 0.6067649350145508\n",
      "287 0.5854142019607971\n",
      "288 0.5648323601663691\n",
      "289 0.5449891402915458\n",
      "290 0.5258541066489776\n",
      "291 0.507405443633585\n",
      "292 0.4896243532083601\n",
      "293 0.47247430581445554\n",
      "294 0.4559380951612688\n",
      "295 0.4399954969298584\n",
      "296 0.424620303594734\n",
      "297 0.4097939483763261\n",
      "298 0.39549409713518663\n",
      "299 0.3817031704737738\n",
      "300 0.36840413404929406\n",
      "301 0.35557948244762827\n",
      "302 0.343210553061954\n",
      "303 0.33127998404389275\n",
      "304 0.3197727791210361\n",
      "305 0.308674946436753\n",
      "306 0.2979669476639519\n",
      "307 0.2876402983496556\n",
      "308 0.27767861862383403\n",
      "309 0.2680678172488864\n",
      "310 0.25879653267474495\n",
      "311 0.24985195815469619\n",
      "312 0.24122423509186425\n",
      "313 0.23290139278100336\n",
      "314 0.2248725039904188\n",
      "315 0.21712386147863025\n",
      "316 0.2096480343740799\n",
      "317 0.20243415983327423\n",
      "318 0.19547333071727804\n",
      "319 0.18875782263378182\n",
      "320 0.18227697847573782\n",
      "321 0.17602276933789157\n",
      "322 0.16998666754088387\n",
      "323 0.1641627794665668\n",
      "324 0.15854431472438313\n",
      "325 0.15312109908658966\n",
      "326 0.1478852059468233\n",
      "327 0.14283187963241306\n",
      "328 0.13795617843265762\n",
      "329 0.13324920165033938\n",
      "330 0.12870597082034757\n",
      "331 0.12432104947229491\n",
      "332 0.12008854345829448\n",
      "333 0.1160032775880773\n",
      "334 0.1120595248254807\n",
      "335 0.10825209633572469\n",
      "336 0.1045774421949699\n",
      "337 0.10102912377227583\n",
      "338 0.09760310995884483\n",
      "339 0.09429568711998815\n",
      "340 0.09110274982875287\n",
      "341 0.0880198855377376\n",
      "342 0.08504431895259718\n",
      "343 0.08217065825034729\n",
      "344 0.07939550558945502\n",
      "345 0.07671586379133165\n",
      "346 0.07412899378927375\n",
      "347 0.07163121562578936\n",
      "348 0.06921836245186681\n",
      "349 0.06688839598584598\n",
      "350 0.06463852555214727\n",
      "351 0.062466612117539067\n",
      "352 0.0603684977759615\n",
      "353 0.058341971037926\n",
      "354 0.0563847528773476\n",
      "355 0.05449429201739066\n",
      "356 0.0526685247178601\n",
      "357 0.05090528509782692\n",
      "358 0.04920243330818391\n",
      "359 0.04755697087540903\n",
      "360 0.0459679934513728\n",
      "361 0.04443295041155885\n",
      "362 0.042949960662045754\n",
      "363 0.04151769323085777\n",
      "364 0.04013379401087821\n",
      "365 0.03879690424098177\n",
      "366 0.037505324756576267\n",
      "367 0.03625763586126173\n",
      "368 0.035052659565856584\n",
      "369 0.03388825493623945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370 0.03276293295316717\n",
      "371 0.03167564347954832\n",
      "372 0.03062519918664968\n",
      "373 0.029610250744839858\n",
      "374 0.02862946290449472\n",
      "375 0.0276817428956946\n",
      "376 0.02676608705170468\n",
      "377 0.025881243019153965\n",
      "378 0.02502623594414074\n",
      "379 0.024200072326679534\n",
      "380 0.023401377442895586\n",
      "381 0.022629562916423117\n",
      "382 0.021883755812634166\n",
      "383 0.021162932484849205\n",
      "384 0.02046635503956351\n",
      "385 0.019793025932585334\n",
      "386 0.019142383048182436\n",
      "387 0.018513279496632985\n",
      "388 0.017905289484600267\n",
      "389 0.017317792880355936\n",
      "390 0.01674977008639568\n",
      "391 0.016200659634127444\n",
      "392 0.015669849142816224\n",
      "393 0.015156799204983836\n",
      "394 0.014660952449955932\n",
      "395 0.014181460614555656\n",
      "396 0.013717953938381554\n",
      "397 0.013269808829601546\n",
      "398 0.012836552717094956\n",
      "399 0.012417828409884056\n",
      "400 0.01201292207066571\n",
      "401 0.011621427808808978\n",
      "402 0.011242980400073006\n",
      "403 0.010877014840857822\n",
      "404 0.010523148507387364\n",
      "405 0.010180951425897828\n",
      "406 0.00985008979522941\n",
      "407 0.009530155686259378\n",
      "408 0.009220823084472952\n",
      "409 0.008921723655109622\n",
      "410 0.008632487165705818\n",
      "411 0.00835272735467554\n",
      "412 0.008082226633239282\n",
      "413 0.00782063786395465\n",
      "414 0.0075676884077517746\n",
      "415 0.007322981232115452\n",
      "416 0.007086326875054115\n",
      "417 0.006857421130254842\n",
      "418 0.006636120950720414\n",
      "419 0.006422068232920538\n",
      "420 0.006214965729905528\n",
      "421 0.006014664993612734\n",
      "422 0.005820935198464817\n",
      "423 0.0056335823547249645\n",
      "424 0.0054523237597001\n",
      "425 0.005276996038897856\n",
      "426 0.005107421140312337\n",
      "427 0.0049433386037853085\n",
      "428 0.004784670374318768\n",
      "429 0.004631135664312563\n",
      "430 0.004482590119525892\n",
      "431 0.004338896379628785\n",
      "432 0.004199884907478107\n",
      "433 0.004065414015055886\n",
      "434 0.0039353249445220105\n",
      "435 0.0038094607796017926\n",
      "436 0.00368765505616644\n",
      "437 0.003569816054376615\n",
      "438 0.0034558170203585984\n",
      "439 0.0033454835383491125\n",
      "440 0.0032387266637218396\n",
      "441 0.0031354678763151223\n",
      "442 0.0030355487949593543\n",
      "443 0.0029388575777628697\n",
      "444 0.002845277367952508\n",
      "445 0.002754718742176385\n",
      "446 0.002667090271059407\n",
      "447 0.002582315906006914\n",
      "448 0.0025002591132535553\n",
      "449 0.0024208494034544236\n",
      "450 0.0023440046283544373\n",
      "451 0.002269621941123319\n",
      "452 0.002197647440393525\n",
      "453 0.0021279819132291383\n",
      "454 0.0020605513215701935\n",
      "455 0.001995294710578359\n",
      "456 0.0019321498604614584\n",
      "457 0.001871042561585842\n",
      "458 0.0018118739315583967\n",
      "459 0.0017546090066179743\n",
      "460 0.001699171501896777\n",
      "461 0.0016455136285984467\n",
      "462 0.0015935870335381714\n",
      "463 0.0015433109432798986\n",
      "464 0.0014946462427916493\n",
      "465 0.0014475427119878081\n",
      "466 0.0014019437264698867\n",
      "467 0.0013577931786128024\n",
      "468 0.0013150603144053506\n",
      "469 0.0012736936859952005\n",
      "470 0.0012336432722242917\n",
      "471 0.0011948812908830597\n",
      "472 0.0011573445095495666\n",
      "473 0.0011210005199065798\n",
      "474 0.001085814696852539\n",
      "475 0.0010517459332572605\n",
      "476 0.0010187680366398684\n",
      "477 0.0009868350789505036\n",
      "478 0.0009559125733076497\n",
      "479 0.0009259751321412464\n",
      "480 0.0008969957583332374\n",
      "481 0.000868929139689065\n",
      "482 0.0008417529421292193\n",
      "483 0.0008154446765062245\n",
      "484 0.0007899618225027823\n",
      "485 0.000765289739509714\n",
      "486 0.0007413956770750466\n",
      "487 0.0007182594233531335\n",
      "488 0.0006958522919996594\n",
      "489 0.0006741551268506085\n",
      "490 0.0006531447157616123\n",
      "491 0.0006327967877627673\n",
      "492 0.0006130888156473061\n",
      "493 0.0005940013313949659\n",
      "494 0.0005755229245967666\n",
      "495 0.0005576290062119596\n",
      "496 0.0005402919600392317\n",
      "497 0.0005235020562551133\n",
      "498 0.0005072389293390039\n",
      "499 0.0004914898529946134\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimensio;\n",
    "# H is hidden dimension; D_out is output dimensin.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialzie weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "iterations = []\n",
    "losses = []\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "    iterations.append(t)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.random.randn(100000)).hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch: Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we introduced the most fundamental PyTorch concept: the **Tensor**. A PyTorch Tensor is indentical to a numpy array: A Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors, Behind the scenes, Tensors can keep track of a computational graph an gradients, but they're also useful as a generic tool for scientific omputing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 456.66546630859375\n",
      "199 1.7432337999343872\n",
      "299 0.01134544238448143\n",
      "399 0.000260656961472705\n",
      "499 4.369526504888199e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predict y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "        \n",
    "    # Backprop to compute gradient of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    \n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytoorch: Tensor and autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `automatic differentiation` to automate the computation of backward passes in neural networks. The `autograd` package is PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a `computation graph`; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you easiy compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 257.0760192871094\n",
      "199 0.8567467331886292\n",
      "299 0.005119595676660538\n",
      "399 0.0001518635544925928\n",
      "499 3.0231831260607578e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden sieze, D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Settings requires_grad=False indicates that we do not need to compute gradients.\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors;\n",
    "    # these are exactly the same operations  we uesed to compute the\n",
    "    # forward pass using Tensors, but we do not need to keep references\n",
    "    # to intermadiate values since we are not implementing the backward\n",
    "    # pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape(1,)\n",
    "    # loss.item() gets the scalar value held in the loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # Use autograd to compute the backward pass. This call will compute\n",
    "    # the gradient of loss with respect to all Tensors with requires_grad=True\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the\n",
    "    # gradient of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manully update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but wee don't need to track\n",
    "    # this in autograd.\n",
    "    # An alternative way is to operate on weights.data and weights.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # Manually zero the gradients after updating weights.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Defining new autograd funtions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, each primitive operator is really two functions that operate on Tensors. The **forward** function computes output Tensors from input Tensors. The **backward** function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch we can easilly define our own autograd operator by defining a subclass of `torch.autograd.Function` and implementing the `forward` and `backward` functions. We can then use our new autograd operator by constructing an instance and calling it like a function, passing Tensors containing input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we define our own custom autograd function for performing the ReLU nonlinearity, and use it to implement our two-layer network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input\n",
    "        and return a Tensor containing the ouput. ctx is context object\n",
    "        that can be used to stash information for backward computation.\n",
    "        You can cache arbitrary objects for usein the backward pass using\n",
    "        the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient\n",
    "        of the loss with respect to the output, and we need to compute\n",
    "        the gradient of the loss with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input<0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 239.82608032226562\n",
      "199 0.7443724870681763\n",
      "299 0.004589669406414032\n",
      "399 0.00015712433378212154\n",
      "499 3.4888969821622595e-05\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensor for weights.\n",
    "x = torch.randn(N, D_in, dtype=dtype, device=device)\n",
    "y = torch.randn(N, D_out, dtype=dtype, device=device)\n",
    "\n",
    "# Create random Tensor for weigths\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method.\n",
    "    relu = MyReLU.apply\n",
    "    \n",
    "    # Forward pass: compute predicted y using operations; we \n",
    "    # compute ReLU using custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # Manually zero the gradient after updating weigths\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational graphs and autograd are a very powerful paradigm for defining complex operations and automatically taking derivatives; however for large neural network raw autograd canbe a bit too low-level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building neuralnetworks we frequently think of arranging the computation into **layers**, some of which have **learnable parameters** which will be optimized during leanrning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the `nn` package defines a set of **Modules**, which are roughly equivalent to neural network layers. A Module receives input Tensors and computes output Tensors, but may also hold internal state such as Tensors containing learnable parameters.Then `nn` package also define a set of useful loss functions that are commonly used when training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the `nn` package to implement our two-layer network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.6743648052215576\n",
      "199 0.050030674785375595\n",
      "299 0.0022794355172663927\n",
      "399 0.00015096722927410156\n",
      "499 1.1786457434936892e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimensin;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers.\n",
    "# nn.Sequential is a Module which contains other Module, and\n",
    "# aplies them in sequence to produce its output. Each Linear\n",
    "# Module computes output from input using a linear function, and\n",
    "# holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss function;\n",
    "# in this case we will use Mean Square Error(MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # Zero the gradients before runing the backward pass.\n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
